apiVersion: apps/v1
kind: Deployment
metadata:
  name: qwen-vl-bench
  labels:
    app: qwen-vl-bench
    engine: vllm
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: qwen-vl-bench
      engine: vllm
  template:
    metadata:
      labels:
        app: qwen-vl-bench
        engine: vllm
    spec:
      nodeSelector:
        # node.kubernetes.io/instance-type: g6e.2xlarge
        karpenter.k8s.aws/instance-family: g6e
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: model
          image: vllm/vllm-openai:latest
          command: ["sleep"]
          args:
            - "infinity"
          ports:
            - name: http
              containerPort: 8000
          resources:
            limits:
              nvidia.com/gpu: "1" # 72B model needs multiple GPUs
          volumeMounts:
            - mountPath: /root/.cache/huggingface
              name: cache-volume
            - name: shm
              mountPath: /dev/shm
          # Remove health checks for benchmark workload as it's not a long-running service
          # env:
          #   - name: CUDA_VISIBLE_DEVICES
          #     value: "0,1,2,3"
      volumes:
        - name: cache-volume
          hostPath:
            path: /mnt/k8s-disks/0/models/qwen-vl-72b
            type: DirectoryOrCreate
        # vLLM needs to access the host's shared memory for tensor parallel inference.
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "20Gi"
---
apiVersion: v1
kind: Service
metadata:
  name: qwen-vl-bench
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app: qwen-vl-bench
    engine: vllm
  type: ClusterIP
